{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"colab":{"name":"models_exploration.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"wt7JHVXQhh1T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621372866069,"user_tz":420,"elapsed":31782,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}},"outputId":"1cdeadac-a709-4d67-8f34-de5a1cd811c6"},"source":["import os\n","import pandas as pd\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","cur_dir = \"/content/drive/MyDrive/Colab_Notebooks/NLP_244_Advanced_ML/final_project_socialiqa/socialiqa-nlp244\"\n","data_dir = \"socialiqa-train-dev\"\n","out_dir = \"out\"\n","\n","!pip install transformers"],"id":"wt7JHVXQhh1T","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/9e/5b80becd952d5f7250eaf8fc64b957077b12ccfe73e9c03d37146ab29712/transformers-4.6.0-py3-none-any.whl (2.3MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 9.0MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 36.4MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 36.0MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting huggingface-hub==0.0.8\n","  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"01c37cd3"},"source":["File for trying out models on SocialIQA"],"id":"01c37cd3"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"78b296fb","executionInfo":{"status":"ok","timestamp":1621372868549,"user_tz":420,"elapsed":12540,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}},"outputId":"70a0b7b7-8afa-4e62-e0d6-777f19b7509b"},"source":["file_train = os.path.join(cur_dir, \"socialiqa-train-dev/train.jsonl\")\n","file_dev = os.path.join(cur_dir, \"socialiqa-train-dev/dev.jsonl\")\n","\n","json_train = pd.read_json(path_or_buf=file_train, lines=True)\n","json_dev = pd.read_json(path_or_buf=file_dev, lines=True)\n","\n","# list of tuples (context, question, A, B++++, C)\n","train_data = [elem for elem in zip(json_train['context'].tolist(), \n","                                   json_train['question'].tolist(), \n","                                   json_train['answerA'].tolist(), \n","                                   json_train['answerB'].tolist(), \n","                                   json_train['answerC'].tolist())]\n","\n","dev_data = [elem for elem in zip(json_dev['context'].tolist(), \n","                                   json_dev['question'].tolist(), \n","                                   json_dev['answerA'].tolist(), \n","                                   json_dev['answerB'].tolist(), \n","                                   json_dev['answerC'].tolist())]\n","\n","len(train_data), len(dev_data)"],"id":"78b296fb","execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(33410, 1954)"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iCy8_aL5C0jN","executionInfo":{"status":"ok","timestamp":1621372869071,"user_tz":420,"elapsed":11413,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}},"outputId":"24802d81-ac9a-406f-89bf-e66dfd0351b6"},"source":["train_labels = []\n","dev_labels = []\n","with open(os.path.join(cur_dir, data_dir, \"train-labels.lst\")) as f:\n","    for line in f:\n","      train_labels.append(int(line.split()[0]))\n","\n","with open(os.path.join(cur_dir, data_dir, \"dev-labels.lst\")) as f:\n","    for line in f:\n","      dev_labels.append(int(line.split()[0]))\n","\n","train_labels = [label-1 for label in train_labels]\n","dev_labels = [label-1 for label in dev_labels]\n","\n","len(train_labels), len(dev_labels)"],"id":"iCy8_aL5C0jN","execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(33410, 1954)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"qwhiVU3OPwrU","executionInfo":{"status":"ok","timestamp":1621373000798,"user_tz":420,"elapsed":5234,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}}},"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import RobertaModel, RobertaTokenizer"],"id":"qwhiVU3OPwrU","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"KOIz47KoR2Np"},"source":["class SocialiqaDataset(Dataset):\n","    \"\"\"\n","    This dataset class for socialiqa might be able to be generalized for \n","    HellaSwag and other tasks.\n","\n","    This is the context/question + multiple choice format, and each example\n","    consists of num choices lists of encoded strings. Note that the input will\n","    be encoded in this stage. prepare_batch will take care of padding across examples\n","    in the batch-level. \n","    \"\"\"\n","    def __init__(self, tokenizer, x, y):\n","        # x: list of tuples containing (context, question, answer1, answer2, answer3)\n","        # y: list of indices of the correct answer\n","        self.roberta_tokenizer = tokenizer\n","        self.x = x\n","        self.y = y\n","\n","    def __getitem__(self, idx):\n","        point = self.x[idx]\n","        input_context_question = [point[0] + self.roberta_tokenizer.sep_token + self.roberta_tokenizer.sep_token + point[1], point[0] + self.roberta_tokenizer.sep_token + self.roberta_tokenizer.sep_token + point[1], point[0] + self.roberta_tokenizer.sep_token + self.roberta_tokenizer.sep_token + point[1]]\n","        input_answers = [point[2], point[3], point[4]]\n","        encoded_text_train = self.roberta_tokenizer(input_context_question, input_answers, return_tensors='pt', padding=True)\n","        return (encoded_text_train, self.y[idx])\n","\n","    def __len__(self):\n","        return len(self.x)\n","\n","\n","def prepare_batch_MC(batch, tokenizer):\n","    \"\"\"\n","    This collate function will pad the batch to be the same length. This requires\n","    flattening, then unflattening for the multiple choice format.\n","    One example will be a list of length 'num choices', each element being a list\n","    of (encoded) tokens representing qustion/answer [sep] choicex\n","    \"\"\"\n","    # batch: [batch_size, (text, label)]\n","    batch_size = len(batch)\n","    print(f\"Batch size: {batch_size}\")\n","\n","    features, labels = zip(*batch)\n","    # features: tuple of length batch_size, \n","    #        each element is a dict with keys = [\"input_ids\", \"attention_mask\"]\n","    # labels: tuple of ints (0, 1, 2) of length batch_size\n","    num_choices = len(features[0][\"input_ids\"])\n","    \n","    # flatten\n","    flattened_features = [\n","            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n","        ]\n","    flattened_features = sum(flattened_features, [])\n","    # flattened_features list length num_choices*batch_size\n","\n","    batch = tokenizer.pad(\n","            flattened_features,\n","            padding=True,\n","            return_tensors=\"pt\",\n","        )\n","    \n","    # Un-flatten\n","    batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","    return (batch, torch.tensor(labels, dtype=torch.int64))\n"],"id":"KOIz47KoR2Np","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C3Nvl8e59rDY"},"source":["tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","dataset_train = SocialiqaDataset(tokenizer, train_data, train_labels)\n","dataset_dev = SocialiqaDataset(tokenizer, dev_data, dev_labels)"],"id":"C3Nvl8e59rDY","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnSNr4MYbjic","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621351488818,"user_tz":420,"elapsed":1310,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}},"outputId":"a74751c9-a326-4cb6-86e5-d9f4f40097f0"},"source":["x = dataset_train.__getitem__(0)[0]\n","y = dataset_train.__getitem__(0)[1]\n","x, y"],"id":"hnSNr4MYbjic","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["({'input_ids': tensor([[    0,   347, 35953,  1276,     7,    33,    10, 18906,     8,  4366,\n","             69,   964,   561,     4,     2,     2,  6179,    74,  5763,   619,\n","             25,    10,   898,   116,     2,     2,  3341,  5190,     2,     1,\n","              1,     1],\n","         [    0,   347, 35953,  1276,     7,    33,    10, 18906,     8,  4366,\n","             69,   964,   561,     4,     2,     2,  6179,    74,  5763,   619,\n","             25,    10,   898,   116,     2,     2,  3341,  4959,   184,     2,\n","              1,     1],\n","         [    0,   347, 35953,  1276,     7,    33,    10, 18906,     8,  4366,\n","             69,   964,   561,     4,     2,     2,  6179,    74,  5763,   619,\n","             25,    10,   898,   116,     2,     2,   102,   205,  1441,     7,\n","             33,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1]])}, 0)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NH2TD7WRb06F","executionInfo":{"status":"ok","timestamp":1621224496238,"user_tz":420,"elapsed":332,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}},"outputId":"5491671d-fb5c-4e94-f3b1-f963fc791fe9"},"source":["print(tokenizer.decode(x['input_ids'][0]), \"\\n\", tokenizer.decode(x['input_ids'][1]), \"\\n\", tokenizer.decode(x['input_ids'][2]))"],"id":"NH2TD7WRb06F","execution_count":null,"outputs":[{"output_type":"stream","text":["<s>Cameron decided to have a barbecue and gathered her friends together.</s></s>How would Others feel as a result?</s></s>like attending</s><pad><pad><pad> \n"," <s>Cameron decided to have a barbecue and gathered her friends together.</s></s>How would Others feel as a result?</s></s>like staying home</s><pad><pad> \n"," <s>Cameron decided to have a barbecue and gathered her friends together.</s></s>How would Others feel as a result?</s></s>a good friend to have</s>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0lTitxJ1JPPU"},"source":["train_loader = DataLoader(dataset_train, batch_size=2, shuffle=True, collate_fn=lambda batch: prepare_batch_MC(batch, tokenizer))\n","#val_loader = DataLoader(dataset_dev, batch_size=1, shuffle=False)\n"],"id":"0lTitxJ1JPPU","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CnQabiyOKFvK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621355934140,"user_tz":420,"elapsed":623,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}},"outputId":"0e5cb7bc-b627-40c8-8572-ab74b48af315"},"source":["for i, batch in enumerate(train_loader):\n","    print(batch)\n","    break\n"],"id":"CnQabiyOKFvK","execution_count":null,"outputs":[{"output_type":"stream","text":["Batch size: 2\n","({'input_ids': tensor([[[    0,   102,  1792,  5460,   439,  5651,    19,    69,  4252,     8,\n","           2037,    10,  3539,    25,    10,  2916,     4,     2,     2,  7608,\n","            222, 17095,  5460,   109,    42,   116,     2,     2,   757, 13447,\n","             69,  4252,     2,     1,     1,     1],\n","         [    0,   102,  1792,  5460,   439,  5651,    19,    69,  4252,     8,\n","           2037,    10,  3539,    25,    10,  2916,     4,     2,     2,  7608,\n","            222, 17095,  5460,   109,    42,   116,     2,     2, 11990,  1531,\n","              2,     1,     1,     1,     1,     1],\n","         [    0,   102,  1792,  5460,   439,  5651,    19,    69,  4252,     8,\n","           2037,    10,  3539,    25,    10,  2916,     4,     2,     2,  7608,\n","            222, 17095,  5460,   109,    42,   116,     2,     2,   225, 20768,\n","              5,  3539,     2,     1,     1,     1]],\n","\n","        [[    0,   104, 15144,   956,     7,   422,    10,  2119, 22379,   463,\n","              8, 18913,    69,  2138,    18,     6,  4224,    18,  4806,     4,\n","              2,     2,  7608,   222, 24737,   109,    42,   116,     2,     2,\n","          30921,  3845,     2,     1,     1,     1],\n","         [    0,   104, 15144,   956,     7,   422,    10,  2119, 22379,   463,\n","              8, 18913,    69,  2138,    18,     6,  4224,    18,  4806,     4,\n","              2,     2,  7608,   222, 24737,   109,    42,   116,     2,     2,\n","           9179,    15,    69,  4806,  5582,     2],\n","         [    0,   104, 15144,   956,     7,   422,    10,  2119, 22379,   463,\n","              8, 18913,    69,  2138,    18,     6,  4224,    18,  4806,     4,\n","              2,     2,  7608,   222, 24737,   109,    42,   116,     2,     2,\n","          20974,    15,     5,   921,     2,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]],\n","\n","        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]])}, tensor([1, 0]))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jF3OMy7TY6Uv"},"source":["class Multiple_Choice_Model(nn.Module):\n","    def __init__(self, roberta_model: RobertaModel, dropout: float):\n","          super(Multiple_Choice_Model, self).__init__()\n","          self.roberta = roberta_model\n","          self.dropout = nn.Dropout(self.roberta.config.hidden_dropout_prob)\n","          self.classifier = nn.Linear(self.config.hidden_size, 1)\n","\n","    def forward(self, input_ids: torch.tensor, attention_mask: torch.tensor, labels=None):\n","          num_choices = input_ids.shape[1] \n","          flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n","          flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n","\n","          outputs = self.roberta(\n","              flat_input_ids,\n","              attention_mask=flat_attention_mask,\n","          )\n","          pooled_output = outputs[1] \n","\n","          pooled_output = self.dropout(pooled_output)\n","          logits = self.classifier(pooled_output)\n","          reshaped_logits = logits.view(-1, num_choices)\n","\n","          loss = None\n","          if labels is not None:\n","              loss_fct = nn.CrossEntropyLoss()\n","              loss = loss_fct(reshaped_logits, labels)\n","\n","          return loss, reshaped_logits\n"],"id":"jF3OMy7TY6Uv","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k4O4_UkdHzu6"},"source":["class Trainer(object):\n","    \"\"\"\n","    Trainer for training a joint multi-label classification and NER model\n","    \"\"\"\n","\n","    def __init__(self, model, optimizer, device=\"cpu\"):\n","        self.model = model.to(device)\n","        self.optimizer = optimizer\n","        self.device = device\n","\n","    def _print_summary(self):\n","        print(self.model)\n","        print(self.optimizer)\n","\n","    def train(self, loader):\n","        \"\"\"\n","        Run a single epoch of training\n","        \"\"\"\n","\n","        self.model.train() # Run model in training mode\n","        slot_loss = None\n","        relation_loss = None\n","        for i, batch in tqdm(enumerate(loader)):\n","            # clear gradient\n","            self.optimizer.zero_grad() \n","\n","            input_ids = batch['input_ids'].to(self.device)\n","            attention_mask = batch['attention_mask'].to(self.device)\n","            slot_labels = batch['slot_labels'].to(self.device)\n","            relation_labels = batch['relation_labels'].to(self.device)\n","            outputs = model(input_ids=input_ids, \n","                            attention_mask=attention_mask,\n","                            relation_labels=relation_labels,\n","                            slot_labels=slot_labels)\n","            slot_loss, relation_loss = outputs[2], outputs[3]\n","            \n","            # back propagation\n","            slot_loss.backward(retain_graph=True) #need to retain_graph  when working with multiple losses\n","            relation_loss.backward()\n","            # do gradient descent\n","            self.optimizer.step() \n","\n","        # Just returning the last loss\n","        return slot_loss, relation_loss\n","\n","    def evaluate(self, loader):\n","        \"\"\"\n","        Evaluate the model on a validation set.\n","        Only do batch size = 1.\n","        \"\"\"\n","\n","        self.model.eval() # Run model in eval mode (disables dropout layer)\n","        slot_loss = None\n","        relation_loss = None\n","        with torch.no_grad(): # Disable gradient computation - required only during training\n","            for i, batch in tqdm(enumerate(loader)):\n","                input_ids = batch['input_ids'].to(self.device)\n","                attention_mask = batch['attention_mask'].to(self.device)\n","                slot_labels = batch['slot_labels'].to(self.device)\n","                relation_labels = batch['relation_labels'].to(self.device)\n","                outputs = model(input_ids=input_ids, \n","                                attention_mask=attention_mask,\n","                                relation_labels=relation_labels,\n","                                slot_labels=slot_labels)\n","                relation_logits, slot_logits = outputs[0], outputs[1]\n","                slot_loss, relation_loss = outputs[2], outputs[3]\n","\n","        # Just returning the last loss\n","        return slot_loss, relation_loss\n","\n","    def get_model_dict(self):\n","        return self.model.state_dict()\n","\n","    def run_training(self, train_loader, valid_loader, n_epochs=3):\n","        # Useful for us to review what experiment we're running\n","        # Normally, you'd want to save this to a file\n","        #self._print_summary()\n","\n","        for i in range(n_epochs):\n","            epoch_slot_loss_train, epoch_relation_loss_train = self.train(train_loader)\n","            epoch_slot_loss_valid, epoch_relation_loss_valid = self.evaluate(valid_loader)\n","            print(f\"Epoch {i}\")\n","            print(f\"Train loss: {epoch_slot_loss_train} (slot), {epoch_relation_loss_train} (relation) \")\n","            print(f\"Valid loss: {epoch_slot_loss_valid} (slot), {epoch_relation_loss_valid} (relation) \")"],"id":"k4O4_UkdHzu6","execution_count":null,"outputs":[]}]}