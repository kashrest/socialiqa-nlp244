{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"colab":{"name":"models_exploration.ipynb","provenance":[],"collapsed_sections":["oS4Y4KYW4dVm"],"toc_visible":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"dcccbdb7c45a479e93f85e6d2cdf36ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5fd1fb8481e341f58e63fc7e5da5fa2f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fb524050a78e47a5bf30898917b230f4","IPY_MODEL_c6486ede85854ba7800b699baf52e60f"]}},"5fd1fb8481e341f58e63fc7e5da5fa2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fb524050a78e47a5bf30898917b230f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_335a53bfecea4e328735c55af8d87114","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0c93731c312a4599b9b9eeb778053a69"}},"c6486ede85854ba7800b699baf52e60f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4571d4795a7d437ca7a590efd10120e4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4177/? [29:13&lt;00:00,  2.38it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f091b839c54e49e29cc48f800390f473"}},"335a53bfecea4e328735c55af8d87114":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0c93731c312a4599b9b9eeb778053a69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4571d4795a7d437ca7a590efd10120e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f091b839c54e49e29cc48f800390f473":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"72c4e7f79d844d63b779cf31c19ca40f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e57d02d2aa1f404b8744b2ad2ee39068","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2681ea09b9804864bb516a6b8f743e06","IPY_MODEL_5e4952f6e47f4075add8c89704a47e19"]}},"e57d02d2aa1f404b8744b2ad2ee39068":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2681ea09b9804864bb516a6b8f743e06":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_149c8bfc8fb84132945fef03e323875e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_23a2a02aa4784c12ac6b6c8cc6945070"}},"5e4952f6e47f4075add8c89704a47e19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d7b2b8436ec3497fb8e533d0ecad6b50","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1954/? [00:55&lt;00:00, 35.36it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7a6d75d8078c495a8cb1775cf4162df4"}},"149c8bfc8fb84132945fef03e323875e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"23a2a02aa4784c12ac6b6c8cc6945070":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d7b2b8436ec3497fb8e533d0ecad6b50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7a6d75d8078c495a8cb1775cf4162df4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"959e29247811443a897eeae709ce1ce6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8a85cd991d514963902fa588c2e459c8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3ebfb72a496e452783333faf7ca2f2f6","IPY_MODEL_2fdbc2eee89549e6911190fbcebf0a0c"]}},"8a85cd991d514963902fa588c2e459c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ebfb72a496e452783333faf7ca2f2f6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e25b0678b2e7474ca1341e7ba7f63d7a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_88ebe1b11f6a4926b5f94c5f960d73f0"}},"2fdbc2eee89549e6911190fbcebf0a0c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5abfe4494c4f41c1a2d651e6684f4e00","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4177/? [30:05&lt;00:00,  2.31it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c824fee386c74483894b204382057191"}},"e25b0678b2e7474ca1341e7ba7f63d7a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"88ebe1b11f6a4926b5f94c5f960d73f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5abfe4494c4f41c1a2d651e6684f4e00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c824fee386c74483894b204382057191":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"305ba0a22b484f8297c4e7bfe229e7dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fece89665c71442bb75eb40e09cb19c1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3947148959184be8a55cda13686415c9","IPY_MODEL_96da69a30c5c4b1a8e6391bb6ce8c343"]}},"fece89665c71442bb75eb40e09cb19c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3947148959184be8a55cda13686415c9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_118392e31ffd4c668a441db2304563c8","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fa81eaa884854cff839e6671762b8515"}},"96da69a30c5c4b1a8e6391bb6ce8c343":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_91a6d51b24f745e1b26748bee11b8a27","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1954/? [00:54&lt;00:00, 35.58it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2596e11d01ff41b7bad9228dc34437f2"}},"118392e31ffd4c668a441db2304563c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fa81eaa884854cff839e6671762b8515":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"91a6d51b24f745e1b26748bee11b8a27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2596e11d01ff41b7bad9228dc34437f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"16952424fcd24d33877b221d8aa00340":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_99db206b428d4c6aaae69873a6b488b6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dc8a65fffcaf40349477b4f60538d339","IPY_MODEL_3cc51026dbd6421892ffe51337ed19ee"]}},"99db206b428d4c6aaae69873a6b488b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dc8a65fffcaf40349477b4f60538d339":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_017024b400f34db494a6d75cf1083ee1","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6d127e9af13341cfa572fadd4d57f5d5"}},"3cc51026dbd6421892ffe51337ed19ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f38993c26acc4577b5699ab013f8d60d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1911/? [13:19&lt;00:00,  2.45it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_51bea6d8e66d42f0b3fcf0115a8f2fd4"}},"017024b400f34db494a6d75cf1083ee1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6d127e9af13341cfa572fadd4d57f5d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f38993c26acc4577b5699ab013f8d60d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"51bea6d8e66d42f0b3fcf0115a8f2fd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"LR58mrJCmGhS"},"source":["# Top"],"id":"LR58mrJCmGhS"},{"cell_type":"code","metadata":{"id":"nAXLBgFil7Wq"},"source":["!pip install transformers\n","from google.colab import drive\n","drive.mount('/content/drive')"],"id":"nAXLBgFil7Wq","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wt7JHVXQhh1T","executionInfo":{"status":"ok","timestamp":1621531925271,"user_tz":420,"elapsed":6483,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}}},"source":["import os\n","import pandas as pd\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset\n","\n","from transformers import RobertaModel, RobertaTokenizer, AdamW\n","\n","from tqdm.auto import tqdm\n","\n","# For visualizations\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","cur_dir = \"/content/drive/MyDrive/Colab_Notebooks/NLP_244_Advanced_ML/final_project_socialiqa/socialiqa-nlp244\"\n","data_dir = \"socialiqa-train-dev\"\n","out_dir = \"out\""],"id":"wt7JHVXQhh1T","execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vwAB6d-Fmqbr"},"source":["# Extract SocialIQA data"],"id":"vwAB6d-Fmqbr"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"78b296fb","executionInfo":{"status":"ok","timestamp":1621531925280,"user_tz":420,"elapsed":6483,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}},"outputId":"52fa1bda-6c7f-404f-91cf-e2250db42a91"},"source":["file_train = os.path.join(cur_dir, \"socialiqa-train-dev/train.jsonl\")\n","file_dev = os.path.join(cur_dir, \"socialiqa-train-dev/dev.jsonl\")\n","\n","json_train = pd.read_json(path_or_buf=file_train, lines=True)\n","json_dev = pd.read_json(path_or_buf=file_dev, lines=True)\n","\n","# list of tuples (context, question, A, B++++, C)\n","train_data = [elem for elem in zip(json_train['context'].tolist(), \n","                                   json_train['question'].tolist(), \n","                                   json_train['answerA'].tolist(), \n","                                   json_train['answerB'].tolist(), \n","                                   json_train['answerC'].tolist())]\n","\n","dev_data = [elem for elem in zip(json_dev['context'].tolist(), \n","                                   json_dev['question'].tolist(), \n","                                   json_dev['answerA'].tolist(), \n","                                   json_dev['answerB'].tolist(), \n","                                   json_dev['answerC'].tolist())]\n","\n","len(train_data), len(dev_data)"],"id":"78b296fb","execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(33410, 1954)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iCy8_aL5C0jN","executionInfo":{"status":"ok","timestamp":1621531925283,"user_tz":420,"elapsed":6475,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}},"outputId":"f94f566b-1f5c-4a0c-9817-f139257b56e7"},"source":["train_labels = []\n","dev_labels = []\n","with open(os.path.join(cur_dir, data_dir, \"train-labels.lst\")) as f:\n","    for line in f:\n","      train_labels.append(int(line.split()[0]))\n","\n","with open(os.path.join(cur_dir, data_dir, \"dev-labels.lst\")) as f:\n","    for line in f:\n","      dev_labels.append(int(line.split()[0]))\n","\n","train_labels = [label-1 for label in train_labels]\n","dev_labels = [label-1 for label in dev_labels]\n","\n","len(train_labels), len(dev_labels)"],"id":"iCy8_aL5C0jN","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(33410, 1954)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"KOIz47KoR2Np","executionInfo":{"status":"ok","timestamp":1621531925941,"user_tz":420,"elapsed":7126,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}}},"source":["class SocialiqaDataset(Dataset):\n","    \"\"\"\n","    This dataset class for socialiqa might be able to be generalized for \n","    HellaSwag and other tasks.\n","\n","    This is the context/question + multiple choice format, and each example\n","    consists of num choices lists of encoded strings. Note that the input will\n","    be encoded in this stage. prepare_batch will take care of padding across examples\n","    in the batch-level. \n","    \"\"\"\n","    def __init__(self, tokenizer, x, y):\n","        # x: list of tuples containing (context, question, answer1, answer2, answer3)\n","        # y: list of indices of the correct answer\n","        self.roberta_tokenizer = tokenizer\n","        self.x = x\n","        self.y = y\n","\n","    def __getitem__(self, idx):\n","        point = self.x[idx]\n","        input_context_question = [point[0] + self.roberta_tokenizer.sep_token + self.roberta_tokenizer.sep_token + point[1], point[0] + self.roberta_tokenizer.sep_token + self.roberta_tokenizer.sep_token + point[1], point[0] + self.roberta_tokenizer.sep_token + self.roberta_tokenizer.sep_token + point[1]]\n","        input_answers = [point[2], point[3], point[4]]\n","        encoded_text_train = self.roberta_tokenizer(input_context_question, input_answers, return_tensors='pt', padding=True)\n","        return (encoded_text_train, self.y[idx])\n","\n","    def __len__(self):\n","        return len(self.x)\n","\n","\n","def prepare_batch_MC(batch, tokenizer):\n","    \"\"\"\n","    This collate function will pad the batch to be the same length. This requires\n","    flattening, then unflattening for the multiple choice format.\n","    One example will be a list of length 'num choices', each element being a list\n","    of (encoded) tokens representing qustion/answer [sep] choicex\n","    \"\"\"\n","    # batch: [batch_size, (text, label)]\n","    batch_size = len(batch)\n","\n","    features, labels = zip(*batch)\n","    # features: tuple of length batch_size, \n","    #        each element is a dict with keys = [\"input_ids\", \"attention_mask\"]\n","    # labels: tuple of ints (0, 1, 2) of length batch_size\n","    num_choices = len(features[0][\"input_ids\"])\n","    \n","    # flatten\n","    flattened_features = [\n","            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n","        ]\n","    flattened_features = sum(flattened_features, [])\n","    # flattened_features list length num_choices*batch_size\n","\n","    batch = tokenizer.pad(\n","            flattened_features,\n","            padding=True,\n","            return_tensors=\"pt\",\n","        )\n","    \n","    # Un-flatten\n","    batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","    return (batch, torch.tensor(labels, dtype=torch.int64))\n"],"id":"KOIz47KoR2Np","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"C3Nvl8e59rDY","executionInfo":{"status":"ok","timestamp":1621531927221,"user_tz":420,"elapsed":8401,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}}},"source":["tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","dataset_train = SocialiqaDataset(tokenizer, train_data, train_labels)\n","dataset_dev = SocialiqaDataset(tokenizer, dev_data, dev_labels)"],"id":"C3Nvl8e59rDY","execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rXeNcECXmluN"},"source":["# Model Class"],"id":"rXeNcECXmluN"},{"cell_type":"code","metadata":{"id":"jF3OMy7TY6Uv","executionInfo":{"status":"ok","timestamp":1621531927230,"user_tz":420,"elapsed":8406,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}}},"source":["class Multiple_Choice_Model(nn.Module):\n","    def __init__(self, roberta_model: RobertaModel, dropout: float = None):\n","          super(Multiple_Choice_Model, self).__init__()\n","          self.roberta = roberta_model\n","          self.dropout = nn.Dropout(self.roberta.config.hidden_dropout_prob)\n","          print(f\"Initializing with hidden size {self.roberta.config.hidden_size}\")\n","          self.classifier = nn.Linear(self.roberta.config.hidden_size, 1)\n","\n","    def forward(self, input_ids: torch.tensor, attention_mask: torch.tensor, labels=None):\n","          num_choices = input_ids.shape[1] \n","          flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n","          flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n","\n","          outputs = self.roberta(\n","              input_ids = flat_input_ids,\n","              attention_mask=flat_attention_mask,\n","          )\n","          pooled_output = outputs[1] \n","\n","          pooled_output = self.dropout(pooled_output)\n","          logits = self.classifier(pooled_output)\n","          reshaped_logits = logits.view(-1, num_choices)\n","\n","          loss_fct = nn.CrossEntropyLoss()\n","          loss = loss_fct(reshaped_logits, labels)\n","\n","          return loss, reshaped_logits"],"id":"jF3OMy7TY6Uv","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"k4O4_UkdHzu6","executionInfo":{"status":"ok","timestamp":1621531927230,"user_tz":420,"elapsed":8401,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}}},"source":["from sklearn.metrics import classification_report\n","\n","class Trainer(object):\n","    \"\"\"\n","    Trainer for training a multiple choice classification model\n","    \"\"\"\n","\n","    def __init__(self, model, optimizer, device=\"cpu\"):\n","        self.model = model.to(device)\n","        self.optimizer = optimizer\n","        self.device = device\n","\n","    def _print_summary(self):\n","        print(self.model)\n","        print(self.optimizer)\n","\n","    def train(self, loader):\n","        \"\"\"\n","        Run a single epoch of training\n","        \"\"\"\n","\n","        self.model.train() # Run model in training mode\n","        loss = None\n","\n","        epoch_true_labels = []\n","        epoch_preds = []\n","        for i, batch in tqdm(enumerate(loader)):\n","            # clear gradient\n","            self.optimizer.zero_grad() \n","\n","            # input_ids shape: (batch_size, num_choices, sequence_length)\n","            input_ids = batch[0]['input_ids'].to(self.device)\n","            # input_ids shape: (batch_size, num_choices, sequence_length)\n","            attention_mask = batch[0]['attention_mask'].to(self.device)\n","            # labels shape: (batch_size, )\n","            labels = batch[1].to(self.device)\n","\n","            outputs = self.model(input_ids=input_ids, \n","                            attention_mask=attention_mask,\n","                            labels=labels)\n","            loss, logits = outputs[0], outputs[1]\n","            \n","            epoch_true_labels.extend(labels.tolist())\n","            epoch_preds.extend(torch.argmax(nn.Softmax(dim=1)(logits), dim=1).tolist())\n","            \n","            # back propagation\n","            loss.backward()\n","            # do gradient descent\n","            self.optimizer.step() \n","\n","        # Just returning the last loss\n","        return loss, epoch_true_labels, epoch_preds\n","\n","    def evaluate(self, loader):\n","        \"\"\"\n","        Evaluate the model on a validation set.\n","        Only do batch size = 1.\n","        \"\"\"\n","\n","        self.model.eval() # Run model in eval mode (disables dropout layer)\n","        loss = None\n","\n","        epoch_true_labels = []\n","        epoch_preds = []\n","        with torch.no_grad(): # Disable gradient computation - required only during training\n","            for i, batch in tqdm(enumerate(loader)):\n","                # input_ids shape: (batch_size, num_choices, sequence_length)\n","                input_ids = batch[0]['input_ids'].to(self.device)\n","                # input_ids shape: (batch_size, num_choices, sequence_length)\n","                attention_mask = batch[0]['attention_mask'].to(self.device)\n","                # labels shape: (batch_size, )\n","                labels = batch[1].to(self.device)\n","\n","                outputs = self.model(input_ids=input_ids, \n","                                attention_mask=attention_mask,\n","                                labels=labels)\n","                loss, logits = outputs[0], outputs[1]\n","                \n","                epoch_true_labels.extend(labels.tolist())\n","                epoch_preds.extend(torch.argmax(nn.Softmax(dim=1)(logits), dim=1).tolist())\n","            \n","        # Just returning the last loss\n","        return loss, epoch_true_labels, epoch_preds\n","\n","    def get_model_dict(self):\n","        return self.model.state_dict()\n","\n","    def run_training(self, train_loader, valid_loader, n_epochs=3):\n","        # Useful for us to review what experiment we're running\n","        # Normally, you'd want to save this to a file\n","        # self._print_summary()\n","        losses_valid = []\n","        losses_train = []\n","        best_valid = float(\"inf\")\n","        for i in range(n_epochs):\n","            target_names = ['Answer A', 'Answer B', 'Answer C']\n","            epoch_loss_train, labels, preds = self.train(train_loader)\n","            print(\"Train eval\")\n","            print(classification_report(labels, preds, target_names=target_names))\n","\n","            epoch_loss_valid, labels, preds = self.evaluate(valid_loader)\n","            print(\"Valid eval\")\n","            print(classification_report(labels, preds, target_names=target_names))\n","\n","\n","            if epoch_loss_valid < best_valid:\n","                best_valid = epoch_loss_valid\n","                torch.save(self.get_model_dict(), os.path.join(cur_dir, out_dir, \"roberta-base-socialiqa\", f'model-mc-checkpoint-epoch{i+1}.pt'))\n","            \n","            losses_train.append(epoch_loss_train.tolist())\n","            losses_valid.append(epoch_loss_valid.tolist())\n","            print(f\"Epoch {i}\")\n","            print(f\"Train loss: {epoch_loss_train}\")\n","            print(f\"Valid loss: {epoch_loss_valid}\")\n","\n","        train_epoch_idx = range(len(losses_train))\n","        valid_epoch_idx = range(len(losses_valid))\n","        # sns.lineplot(epoch_idx, all_losses)\n","        sns.lineplot(train_epoch_idx, losses_train)\n","        sns.lineplot(valid_epoch_idx, losses_valid)\n","        plt.show()"],"id":"k4O4_UkdHzu6","execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QyKRRNydoJ3","executionInfo":{"status":"ok","timestamp":1621531929836,"user_tz":420,"elapsed":11001,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}},"outputId":"f3d9fbfa-834b-4fc7-ac92-b886ab7f5420"},"source":["roberta_base = RobertaModel.from_pretrained('roberta-base')\n","mc_model = Multiple_Choice_Model(roberta_base)\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# 'W' stands for 'Weight Decay fix\"\n","# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","optimizer = AdamW(mc_model.parameters(), lr=1e-5)"],"id":"2QyKRRNydoJ3","execution_count":9,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["Initializing with hidden size 768\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0lTitxJ1JPPU","executionInfo":{"status":"ok","timestamp":1621531929843,"user_tz":420,"elapsed":11002,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}}},"source":["train_loader = DataLoader(dataset_train, batch_size=8, shuffle=True, collate_fn=lambda batch: prepare_batch_MC(batch, tokenizer))\n","val_loader = DataLoader(dataset_dev, batch_size=1, shuffle=False)"],"id":"0lTitxJ1JPPU","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JbRMuMxpqehU"},"source":["# Training"],"id":"JbRMuMxpqehU"},{"cell_type":"code","metadata":{"id":"n_xhloZnfXal","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["dcccbdb7c45a479e93f85e6d2cdf36ce","5fd1fb8481e341f58e63fc7e5da5fa2f","fb524050a78e47a5bf30898917b230f4","c6486ede85854ba7800b699baf52e60f","335a53bfecea4e328735c55af8d87114","0c93731c312a4599b9b9eeb778053a69","4571d4795a7d437ca7a590efd10120e4","f091b839c54e49e29cc48f800390f473","72c4e7f79d844d63b779cf31c19ca40f","e57d02d2aa1f404b8744b2ad2ee39068","2681ea09b9804864bb516a6b8f743e06","5e4952f6e47f4075add8c89704a47e19","149c8bfc8fb84132945fef03e323875e","23a2a02aa4784c12ac6b6c8cc6945070","d7b2b8436ec3497fb8e533d0ecad6b50","7a6d75d8078c495a8cb1775cf4162df4","959e29247811443a897eeae709ce1ce6","8a85cd991d514963902fa588c2e459c8","3ebfb72a496e452783333faf7ca2f2f6","2fdbc2eee89549e6911190fbcebf0a0c","e25b0678b2e7474ca1341e7ba7f63d7a","88ebe1b11f6a4926b5f94c5f960d73f0","5abfe4494c4f41c1a2d651e6684f4e00","c824fee386c74483894b204382057191","305ba0a22b484f8297c4e7bfe229e7dc","fece89665c71442bb75eb40e09cb19c1","3947148959184be8a55cda13686415c9","96da69a30c5c4b1a8e6391bb6ce8c343","118392e31ffd4c668a441db2304563c8","fa81eaa884854cff839e6671762b8515","91a6d51b24f745e1b26748bee11b8a27","2596e11d01ff41b7bad9228dc34437f2","16952424fcd24d33877b221d8aa00340","99db206b428d4c6aaae69873a6b488b6","dc8a65fffcaf40349477b4f60538d339","3cc51026dbd6421892ffe51337ed19ee","017024b400f34db494a6d75cf1083ee1","6d127e9af13341cfa572fadd4d57f5d5","f38993c26acc4577b5699ab013f8d60d","51bea6d8e66d42f0b3fcf0115a8f2fd4"]},"outputId":"c238142d-e3f6-4c0b-da5c-3bc18cf12914"},"source":["trainer_socialiqa = Trainer(mc_model, optimizer, device)\n","trainer_socialiqa.run_training(train_loader, val_loader, n_epochs=10)"],"id":"n_xhloZnfXal","execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dcccbdb7c45a479e93f85e6d2cdf36ce","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Train eval\n","              precision    recall  f1-score   support\n","\n","    Answer A       0.71      0.70      0.70     11274\n","    Answer B       0.70      0.70      0.70     11176\n","    Answer C       0.70      0.71      0.70     10960\n","\n","    accuracy                           0.70     33410\n","   macro avg       0.70      0.70      0.70     33410\n","weighted avg       0.70      0.70      0.70     33410\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72c4e7f79d844d63b779cf31c19ca40f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Valid eval\n","              precision    recall  f1-score   support\n","\n","    Answer A       0.67      0.68      0.67       643\n","    Answer B       0.68      0.64      0.66       654\n","    Answer C       0.68      0.71      0.69       657\n","\n","    accuracy                           0.68      1954\n","   macro avg       0.68      0.68      0.68      1954\n","weighted avg       0.68      0.68      0.68      1954\n","\n","Epoch 0\n","Train loss: 0.6505564451217651\n","Valid loss: 0.12656888365745544\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"959e29247811443a897eeae709ce1ce6","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Train eval\n","              precision    recall  f1-score   support\n","\n","    Answer A       0.80      0.80      0.80     11274\n","    Answer B       0.81      0.79      0.80     11176\n","    Answer C       0.79      0.81      0.80     10960\n","\n","    accuracy                           0.80     33410\n","   macro avg       0.80      0.80      0.80     33410\n","weighted avg       0.80      0.80      0.80     33410\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"305ba0a22b484f8297c4e7bfe229e7dc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Valid eval\n","              precision    recall  f1-score   support\n","\n","    Answer A       0.71      0.71      0.71       643\n","    Answer B       0.70      0.68      0.69       654\n","    Answer C       0.70      0.72      0.71       657\n","\n","    accuracy                           0.70      1954\n","   macro avg       0.70      0.70      0.70      1954\n","weighted avg       0.70      0.70      0.70      1954\n","\n","Epoch 1\n","Train loss: 0.37591660022735596\n","Valid loss: 0.04180977866053581\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"16952424fcd24d33877b221d8aa00340","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"3yLG4Z3Zsfik","executionInfo":{"status":"aborted","timestamp":1621531907670,"user_tz":420,"elapsed":41491,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}}},"source":["file_name = 'model-mc-checkpoint-.pt'\n","torch.save(mc_model.state_dict(), os.path.join(cur_dir, out_dir, file_name))"],"id":"3yLG4Z3Zsfik","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5bMwwi5SrmNu"},"source":["# Evaluation"],"id":"5bMwwi5SrmNu"},{"cell_type":"code","metadata":{"id":"nW_fMebsnw1C","executionInfo":{"status":"aborted","timestamp":1621531907671,"user_tz":420,"elapsed":41486,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}}},"source":["mc_model_restored_state_dict = torch.load(os.path.join(cur_dir, out_dir, 'model-mc-checkpoint-epoch10.pt'), )\n","\n","roberta_base = RobertaModel.from_pretrained('roberta-base')\n","mc_model_restored = Multiple_Choice_Model(roberta_base)\n","\n","# Restore state dict to load the same weights again\n","mc_model_restored.load_state_dict(mc_model_restored_state_dict)"],"id":"nW_fMebsnw1C","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S4en2rcQD4GH","executionInfo":{"status":"aborted","timestamp":1621531907672,"user_tz":420,"elapsed":41483,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}}},"source":["mc_model_restored.to(device)\n","mc_model_restored.eval()\n","val_loader = DataLoader(dataset_dev, batch_size=1, shuffle=False)\n","for i, batch in tqdm(enumerate(val_loader)):\n","                # input_ids shape: (batch_size, num_choices, sequence_length)\n","                input_ids = batch[0]['input_ids'].to(device)\n","                # input_ids shape: (batch_size, num_choices, sequence_length)\n","                attention_mask = batch[0]['attention_mask'].to(device)\n","                # labels shape: (batch_size, )\n","                labels = batch[1].to(device)\n","\n","                outputs = mc_model_restored(input_ids=input_ids, \n","                                attention_mask=attention_mask,\n","                                labels=labels)\n","                _, logits = outputs[0], outputs[1]\n","                print(logits)\n","                "],"id":"S4en2rcQD4GH","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oS4Y4KYW4dVm"},"source":["# Debugging with pretrained Roberta for MC"],"id":"oS4Y4KYW4dVm"},{"cell_type":"code","metadata":{"id":"XiMBzT565YNk","executionInfo":{"status":"aborted","timestamp":1621531907673,"user_tz":420,"elapsed":41477,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}}},"source":["tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","model = RobertaForMultipleChoice.from_pretrained('roberta-base')\n","\n","context = \"We are talking about pizza\"\n","prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n","choice0 = \"It is eaten with a fork and a knife.\"\n","choice1 = \"It is eaten while held in the hand.\"\n","labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n","\n","encoding = tokenizer([[prompt, prompt], [choice0, choice1]], return_tensors='pt', padding=True)\n","outputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels)  # batch size is 1\n","\n","# the linear classifier still needs to be trained\n","loss = outputs.loss\n","logits = outputs.logits\n","print(logits)"],"id":"XiMBzT565YNk","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MdOTbBCO5eGq","executionInfo":{"status":"aborted","timestamp":1621531907676,"user_tz":420,"elapsed":41478,"user":{"displayName":"Kaleen Shrestha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjok_63OtbONf0JKntnYC4yBHRKmSHha6vk0nl7uA=s64","userId":"17677293658466379584"}}},"source":[""],"id":"MdOTbBCO5eGq","execution_count":null,"outputs":[]}]}